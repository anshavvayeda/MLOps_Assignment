{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshavvayeda/MLOps_Assignment/blob/main/Ml_Ops_Assignment_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "imimportport torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.quantization\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "9u14XV1H6bGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "digits = load_digits()\n",
        "\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "cbuYDyz-7llR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PyTorchLogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(PyTorchLogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = len(np.unique(y_train))  # Number of classes\n",
        "pytorch_model = PyTorchLogisticRegression(input_dim, output_dim)\n"
      ],
      "metadata": {
        "id": "CTYsWJ057nzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(pytorch_model.parameters(), lr=0.01)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = pytorch_model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "sBDfDujc7sh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure inference time for the original model\n",
        "start_inference = time.time()\n",
        "with torch.no_grad():\n",
        "    test_outputs = pytorch_model(X_test_tensor)\n",
        "    _, predicted = torch.max(test_outputs, 1)\n",
        "    inference_time = time.time() - start_inference\n",
        "\n",
        "# Calculate accuracy\n",
        "original_accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n",
        "\n",
        "# Calculate model size\n",
        "original_model_size = len(pickle.dumps(pytorch_model))\n",
        "\n",
        "# Display original model results\n",
        "print(f\"Original Model Accuracy: {original_accuracy:.4f}\")\n",
        "print(f\"Original Model Size: {original_model_size} bytes\")\n",
        "print(f\"Original Model Inference Time: {inference_time:.6f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O759TQf7tu8",
        "outputId": "ded33808-40ba-4411-e62e-8c3ec4f9ff4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Accuracy: 0.7556\n",
            "Original Model Size: 4048 bytes\n",
            "Original Model Inference Time: 0.022371 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply dynamic quantization\n",
        "quantized_pytorch_model = torch.quantization.quantize_dynamic(\n",
        "    pytorch_model, {nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Measure quantized model inference time\n",
        "start_inference_quantized = time.time()\n",
        "with torch.no_grad():\n",
        "    quantized_outputs = quantized_pytorch_model(X_test_tensor)\n",
        "    _, quantized_predicted = torch.max(quantized_outputs, 1)\n",
        "    quantized_inference_time = time.time() - start_inference_quantized\n",
        "\n",
        "# Calculate quantized model accuracy\n",
        "quantized_accuracy = (quantized_predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n",
        "\n",
        "# Calculate the quantized model size\n",
        "quantized_model_size = len(pickle.dumps(quantized_pytorch_model))\n",
        "\n",
        "# Display quantized model results\n",
        "print(f\"Quantized Model Accuracy: {quantized_accuracy:.4f}\")\n",
        "print(f\"Quantized Model Size: {quantized_model_size} bytes\")\n",
        "print(f\"Quantized Model Inference Time: {quantized_inference_time:.6f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QkHc9IO7wdD",
        "outputId": "3e905ab2-3b61-480d-dcf7-7f5a013c4c08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized Model Accuracy: 0.7528\n",
            "Quantized Model Size: 2537 bytes\n",
            "Quantized Model Inference Time: 0.045655 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Comparison\n",
        "print(f\"\\nModel Comparison:\")\n",
        "print(f\"Metric                   Original Model      Quantized Model\")\n",
        "print(f\"------------------------------------------------------------\")\n",
        "print(f\"Accuracy                 {original_accuracy:.4f}              {quantized_accuracy:.4f}\")\n",
        "print(f\"Model Size (bytes)       {original_model_size}                {quantized_model_size}\")\n",
        "print(f\"Inference Time (s)       {inference_time:.6f}          {quantized_inference_time:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXEXnKhe8lEH",
        "outputId": "a1a4fe7e-ff7f-4621-84d2-cb979bfc705c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Comparison:\n",
            "Metric                   Original Model      Quantized Model\n",
            "------------------------------------------------------------\n",
            "Accuracy                 0.7556              0.7528\n",
            "Model Size (bytes)       4048                2537\n",
            "Inference Time (s)       0.022371          0.045655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9DXB9ob78n5q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}